"""Задание 1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1im3PB2OnjFTmUe7KneaPbHFSv3w7Reyn
"""

# Commented out IPython magic to ensure Python compatibility. Я БАРИНОВ ДМИТРИЙ ББСО-01-20
import pandas
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn import metrics
import matplotlib.pyplot as plt
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
import scipy 
from sklearn import metrics
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, mean_absolute_error, precision_recall_curve, average_precision_score, roc_curve, roc_auc_score
# %pylab inline
data = pandas.read_csv("https://raw.githubusercontent.com/SergUSProject/IntelligentSystemsAndTechnologies/main/Practice/datasets/train.csv")

data = pandas.read_csv('https://raw.githubusercontent.com/sdukshis/ml-intro/master/datasets/Davis.csv', index_col=0)
data.head()

X = data[['weight']]
y = data['height']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

regressor = LinearRegression()
regressor.fit(X_train, y_train)
y_pred = regressor.predict(X_test)
datapr = pandas.DataFrame({'True': y_test, 'Predicted': y_pred})
print(datapr.head())
print('MAE:', metrics.mean_absolute_error(y_test, y_pred))
print('MSE:', metrics.mean_squared_error(y_test, y_pred))
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))

X = data[['weight', 'sex', 'repwt']]
y = data['height']

X.dropna()
X_enc = OneHotEncoder().fit(X).transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_enc, y, test_size=0.2, random_state=0)

regressor = LinearRegression()
regressor.fit(X_train, y_train)
y_pred = regressor.predict(X_test)
datapr = pandas.DataFrame({'Actual': y_test, 'Predicted': y_pred})
print(datapr.head())
print('MAE:', metrics.mean_absolute_error(y_test, y_pred))
print('MSE:', metrics.mean_squared_error(y_test, y_pred))
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))

p1 = data['repht'].isna().sum() / data['repht'].shape[0] * 100
p2 = data['repwt'].isna().sum() / data['repwt'].shape[0] * 100
print("% NaN in repht:", p1)
print("% NaN in repwt:", p2)
data = data.dropna()
print("NaN cleared")

y = data['height']
r1, p = scipy.stats.pearsonr(data['weight'], y)
print("Correlation coefficient weight: ", r1)
r2, p = scipy.stats.pearsonr(data['repwt'], y)
print("Correlation coefficient repwt: ", r2)
r3, p = scipy.stats.pearsonr(data['repht'], y)
print("Correlation coefficient repht: ", r3)

X = data[['sex', 'repht']]
y = data['height']

X = OneHotEncoder().fit(X).transform(X)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

DTR = DecisionTreeRegressor() 
DTR.fit(X_train, y_train)
y_pred = DTR.predict(X_test)
datapr = pandas.DataFrame({'Actual': y_test, 'Predicted': y_pred})
print(datapr.head())
print('MAE:', metrics.mean_absolute_error(y_test, y_pred))
print('MSE:', metrics.mean_squared_error(y_test, y_pred))
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))

RFR = RandomForestRegressor() 
RFR.fit(X_train, y_train)
y_pred = RFR.predict(X_test)
datapr = pandas.DataFrame({'Actual': y_test, 'Predicted': y_pred})
print(datapr.head())
print('MAbsolute Error:', metrics.mean_absolute_error(y_test, y_pred))
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))

X, y = make_classification(
    n_samples = 100,
    n_features = 2,
    n_informative = 2,
    n_redundant = 0,
    n_repeated = 0,
    n_classes = 2,
    n_clusters_per_class = 1,
    weights = (0.2, 0.8),
    class_sep = 6.0,
    hypercube = False,
    random_state = 2,
)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

fig, axs = plt.subplots(1, 2)
axs[0].pie([list(y_train).count(1), list(y_train).count(0)], labels=['in', 'out'])
axs[0].set_title('train')
axs[1].pie([list(y_test).count(1), list(y_test).count(0)], labels=['in', 'out'])
axs[1].set_title('test')
plt.show()

LR= LogisticRegression()
LR.fit(X_train, y_train)
y_pred = LR.predict(X_test)
print(classification_report(y_test, y_pred))
print('Accuracy:', 1 - mean_absolute_error(y_test, y_pred))

KNC = KNeighborsClassifier()
KNC.fit(X_train, y_train)
y_pred = KNC.predict(X_test)
print(classification_report(y_test, y_pred))
print('AccuracyN:', 1 - mean_absolute_error(y_test, y_pred))
