#Automatically generated by Colaboratory.

#Original file is located at
#    https://colab.research.google.com/drive/1roZZk0CUOWZaHxTZVfYsy2RPP1cpP_3Z

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd

dataset = pd.read_csv("https://raw.githubusercontent.com/SergUSProject/IntelligentSystemsAndTechnologies/main/Practice/datasets/train.csv")
dataset.head()

dataset = pd.read_csv('https://raw.githubusercontent.com/sdukshis/ml-intro/master/datasets/Davis.csv', index_col=0)
dataset.head()

from sklearn.linear_model import LinearRegression

X = dataset[['weight']]
y = dataset['height']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

regressor = LinearRegression()
regressor.fit(X_train, y_train)
y_pred = regressor.predict(X_test)
df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
print(df.head())
from sklearn import metrics
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))

X = dataset[['sex', 'repht']] 
y = dataset['height']

from sklearn.preprocessing import OneHotEncoder
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
ohe = OneHotEncoder()
ohe.fit(X)
X = ohe.transform(X)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

RFR = RandomForestRegressor() 
RFR.fit(X_train, y_train)
y_pred = RFR.predict(X_test)
df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
print(df.head())
from sklearn import metrics
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))

DTR = DecisionTreeRegressor() 
DTR.fit(X_train, y_train)
y_pred = DTR.predict(X_test)
df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
print(df.head())
from sklearn import metrics
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))

from sklearn.linear_model import LogisticRegression #старт классификации
from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, mean_absolute_error, precision_recall_curve, average_precision_score, roc_curve, roc_auc_score

X, y = make_classification(
    n_samples = 1000,
    n_features = 2,
    n_informative = 2,
    n_redundant = 0,
    n_repeated = 0,
    n_classes = 2,
    n_clusters_per_class = 1,
    weights = (0.15, 0.85),
    class_sep = 6.0,
    hypercube = False,
    random_state = 2,
)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

LR= LogisticRegression()
LR.fit(X_train, y_train)
y_pred = LR.predict(X_test)
print(classification_report(y_test, y_pred))
print('Correct answers log reg:', 1 - mean_absolute_error(y_test, y_pred))

print('Log Regression err matrix:')
print(confusion_matrix(y_test, y_pred))

KNC = KNeighborsClassifier()
KNC.fit(X_train, y_train)
y_pred = KNC.predict(X_test)
print(classification_report(y_test, y_pred))
print('Part of correct answers KNN:', 1 - mean_absolute_error(y_test, y_pred))

print('Err matrix for KNN:')
print(confusion_matrix(y_test, y_pred))
